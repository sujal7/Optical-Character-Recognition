{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWe_tWYXeQL1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn06_3xWU7Lk"
   },
   "outputs": [],
   "source": [
    "def load_az_dataset(datasetPath):\n",
    "    # initialize the list of data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the rows of the A-Z handwritten digit dataset\n",
    "    for row in open(datasetPath):\n",
    "        # parse the label and image from the row\n",
    "        row = row.split(\",\")\n",
    "        label = int(row[0])\n",
    "        image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "\n",
    "        # images are represented as single channel (grayscale) images\n",
    "        # that are 28x28=784 pixels -- we need to take this flattened\n",
    "        # 784-d list of numbers and repshape them into a 28x28 matrix\n",
    "        image = image.reshape((28, 28))\n",
    "\n",
    "        # update the list of data and labels\n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    # convert the data and labels to NumPy arrays\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "    labels = np.array(labels, dtype=\"int\")\n",
    "\n",
    "    # return a 2-tuple of the A-Z data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE_XIiwGeKND"
   },
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    # load the MNIST dataset and stack the training data and testing\n",
    "    # data together (we'll create our own training and testing splits\n",
    "    # later in the project)\n",
    "    ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n",
    "    data = np.vstack([trainData, testData])\n",
    "    labels = np.hstack([trainLabels, testLabels])\n",
    "\n",
    "    # return a 2-tuple of the MNIST data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OViWJ4H9eNPh"
   },
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python train_resnet_model.py --az a_z_handwritten_data.csv --model handwriting.model\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "# from datasets.load_dataset import load_mnist_dataset\n",
    "# from datasets.load_dataset import load_az_dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m3z0lsvef9B"
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-1\n",
    "BS = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u33W8VMPfPwx",
    "outputId": "c5f14932-ed15-4a08-a624-4f950622b88b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading datasets...\n"
     ]
    }
   ],
   "source": [
    "# load the A-Z and MNIST datasets, respectively\n",
    "print(\"[INFO] loading datasets...\")\n",
    "(azData, azLabels) = load_az_dataset(\"/content/drive/MyDrive/dataset/a_z_handwritten_data.csv\") # upload csv file in your drive and give path here\n",
    "(digitsData, digitsLabels) = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k25f7KuGfYrY"
   },
   "outputs": [],
   "source": [
    "# the MNIST dataset occupies the labels 0-9, so let's add 10 to every\n",
    "# A-Z label to ensure the A-Z characters are not incorrectly labeled\n",
    "# as digits\n",
    "azLabels += 10\n",
    "\n",
    "# stack the A-Z data and labels with the MNIST digits data and labels\n",
    "data = np.vstack([azData, digitsData])\n",
    "labels = np.hstack([azLabels, digitsLabels])\n",
    "\n",
    "# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# however, the architecture we're using is designed for 32x32 images,\n",
    "# so we need to resize them to 32x32\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAUTlMmOf7yo"
   },
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "\tclassWeight[i] = classTotals.max() / classTotals[i]\n",
    "\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71BLYV6qgKe4"
   },
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=10,\n",
    "\tzoom_range=0.05,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMhE1wKhh6DA"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class CNN:\n",
    "    def residual_module(data, K, stride, chanDim, red=False,\n",
    "                        reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        # the shortcut branch of the CNN module should be\n",
    "        # initialize as the input (identity) data\n",
    "        shortcut = data\n",
    "\n",
    "        # the first block of the CNN module are the 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "                                 momentum=bnMom)(data)\n",
    "        act1 = Activation(\"relu\")(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "                       kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # the second block of the CNN module are the 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "                                 momentum=bnMom)(conv1)\n",
    "        act2 = Activation(\"relu\")(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "                       padding=\"same\", use_bias=False,\n",
    "                       kernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "        # the third block of the CNN module is another set of 1x1\n",
    "        # CONVs\n",
    "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "                                 momentum=bnMom)(conv2)\n",
    "        act3 = Activation(\"relu\")(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "                       kernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "        # if we are to reduce the spatial size, apply a CONV layer to\n",
    "        # the shortcut\n",
    "        if red:\n",
    "            shortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "                              use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        # add together the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "\n",
    "        # return the addition as the output of the CNN module\n",
    "        return x\n",
    "\n",
    "    def build(width, height, depth, classes, stages, filters,\n",
    "              reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "\n",
    "        # set the input and then apply a BN followed by CONV\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "                               momentum=bnMom)(inputs)\n",
    "        x = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "                   padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "        # loop over the number of stages\n",
    "        for i in range(0, len(stages)):\n",
    "            # initialize the stride, then apply a residual module\n",
    "            # used to reduce the spatial size of the input volume\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "            x = CNN.residual_module(x, filters[i + 1], stride,\n",
    "                                       chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "            # loop over the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a CNN module\n",
    "                x = CNN.residual_module(x, filters[i + 1],\n",
    "                                           (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "        # apply BN => ACT => POOL\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "                               momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"cnn\")\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpdU9kiwgXoA",
    "outputId": "196a050c-88a2-487e-c655-21bdc4949aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# initialize and compile our deep neural network\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = CNN.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "dT-WOf8Wg8FL",
    "outputId": "da3fe862-84c1-48ba-ecbf-f50e3690f484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7a92d1af83de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m H = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=2,\n",
    "\tepochs=10,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)\n",
    "\n",
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(args[\"model\"], save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "cGqIFk9NiaG_",
    "outputId": "c58e357a-3fa8-4621-c9b9-7567c19ca600"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6ceb871f8661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# construct a plot that plots and saves the training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC3DBTBsZ_e7"
   },
   "outputs": [],
   "source": [
    "# randomly select a few testing characters\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
    "\t# classify the character\n",
    "\tprobs = model.predict(testX[np.newaxis, i])\n",
    "\tprediction = probs.argmax(axis=1)\n",
    "\tlabel = labelNames[prediction[0]]\n",
    "\n",
    "\t# extract the image from the test data and initialize the text\n",
    "\t# label color as green (correct)\n",
    "\timage = (testX[i] * 255).astype(\"uint8\")\n",
    "\tcolor = (0, 255, 0)\n",
    "\n",
    "\t# otherwise, the class label prediction is incorrect\n",
    "\tif prediction[0] != np.argmax(testY[i]):\n",
    "\t\tcolor = (0, 0, 255)\n",
    "\n",
    "\t# merge the channels into one image, resize the image from 32x32\n",
    "\t# to 96x96 so we can better see it and then draw the predicted\n",
    "\t# label on the image\n",
    "\timage = cv2.merge([image] * 3)\n",
    "\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "\t\tcolor, 2)\n",
    "\n",
    "\t# add the image to our list of output images\n",
    "\timages.append(image)\n",
    "\n",
    "# construct the montage for the images\n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "\n",
    "# show the output montage\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
